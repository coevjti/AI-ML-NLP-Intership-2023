{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# OOPs implementation of the TTPs prediction model\n",
        "\n"
      ],
      "metadata": {
        "id": "E304w2S4I-J0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All the project related model data : https://drive.google.com/drive/folders/1-6iMy8K-tFxtaFSxxmpnhhjDfAgUQUzp?usp=sharing  "
      ],
      "metadata": {
        "id": "h7aQei7_Iwu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import tensorflow_hub as hub\n",
        "from heapq import nlargest\n",
        "from string import punctuation"
      ],
      "metadata": {
        "id": "shjZK7XcGSbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jF-Hl5UGAk4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class TTPS_prediction:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.ttps_Data = pd.read_csv(\"/content/drive/MyDrive/CyberSecurity/Task_3_TTPS/Complete_Data.csv\")\n",
        "        self.sentenceEncoder_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "        self.attackOrNot_Model = joblib.load(\"/content/drive/MyDrive/CyberSecurity/Task_3_TTPS/AttackOrNot.joblib\")\n",
        "        self.ttp_embeddings = joblib.load(\"/content/drive/MyDrive/CyberSecurity/Task_3_TTPS/ttp_embeddings.pkl\")\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "\n",
        "    def read_paragraphs_from_file(self,file_path):\n",
        "        try:\n",
        "          with open(file_path, 'r',encoding=\"UTF-8\") as file:\n",
        "              content = file.read()\n",
        "              paragraphs = content.split('\\n\\n')\n",
        "              paragraphs = [paragraph.strip() for paragraph in paragraphs if paragraph.strip() and len(paragraph.split())<=400]\n",
        "          self.vectorizationAndEmbedding(paragraphs)\n",
        "        except :\n",
        "          print(\"XXXXXXXXXXXXXX File is not present at give path XXXXXXXXXXX\")\n",
        "\n",
        "\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        # Apply any preprocessing steps (e.g., removing special characters, lowercasing)\n",
        "        string = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "        # Replace email addresses with 'email'\n",
        "        email_regex = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b')\n",
        "        string = email_regex.sub('email', string)\n",
        "\n",
        "        # Replace IP addresses with 'ip'\n",
        "        ip_regex = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n",
        "        string = ip_regex.sub('ip', string)\n",
        "\n",
        "        # Remove newline characters\n",
        "        string = string.replace('\\n', '')\n",
        "        soup = BeautifulSoup(string, \"html.parser\")\n",
        "        string = soup.get_text()\n",
        "\n",
        "        # Remove special characters\n",
        "        string = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", string)\n",
        "\n",
        "        # Remove extra whitespaces\n",
        "        string = re.sub(r\"\\s+\", \" \", string)\n",
        "\n",
        "        preprocessed_text = string.strip()  # Remove leading/trailing whitespaces\n",
        "        return preprocessed_text\n",
        "\n",
        "    def summarize(self, text, per):\n",
        "        doc = self.nlp(text)\n",
        "        tokens = [token.text for token in doc]\n",
        "        word_frequencies = {}\n",
        "        for word in doc:\n",
        "            if word.text.lower() not in list(STOP_WORDS):\n",
        "                if word.text.lower() not in punctuation:\n",
        "                    if word.text not in word_frequencies.keys():\n",
        "                        word_frequencies[word.text] = 1\n",
        "                    else:\n",
        "                        word_frequencies[word.text] += 1\n",
        "        max_frequency = max(word_frequencies.values())\n",
        "        for word in word_frequencies.keys():\n",
        "            word_frequencies[word] = word_frequencies[word] / max_frequency\n",
        "        sentence_tokens = [sent for sent in doc.sents]\n",
        "        sentence_scores = {}\n",
        "        for sent in sentence_tokens:\n",
        "            for word in sent:\n",
        "                if word.text.lower() in word_frequencies.keys():\n",
        "                    if sent not in sentence_scores.keys():\n",
        "                        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
        "                    else:\n",
        "                        sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
        "        select_length = int(len(sentence_tokens) * per)\n",
        "        summary = nlargest(select_length, sentence_scores, key=sentence_scores.get)\n",
        "        final_summary = [word.text for word in summary]\n",
        "        summary = ''.join(final_summary)\n",
        "        return summary\n",
        "\n",
        "    def most_similar_string(self, sentence, string_list):\n",
        "        \"\"\"\n",
        "        Returns the most similar string in the string list to the given sentence.\n",
        "\n",
        "        Args:\n",
        "          sentence: The sentence to compare.\n",
        "          string_list: A list of strings.\n",
        "\n",
        "        Returns:\n",
        "          The most similar string in the string list.\n",
        "        \"\"\"\n",
        "\n",
        "        tfidf_matrix = self.vectorizer.transform(string_list + [sentence])\n",
        "\n",
        "        sentence_embeddings = self.sentenceEncoder_model([sentence])[0]\n",
        "        string_embeddings = self.sentenceEncoder_model(string_list)\n",
        "\n",
        "        cosine_similarities = cosine_similarity([sentence_embeddings], string_embeddings)[0]\n",
        "\n",
        "        most_similar_index = np.argmax(cosine_similarities)\n",
        "        return string_list[most_similar_index]\n",
        "\n",
        "    def vectorizationAndEmbedding(self, paragraphs):\n",
        "        preprocessed_paragraphs = [self.preprocess_text(paragraph) for paragraph in paragraphs]\n",
        "\n",
        "        # Fit the vectorizer on the preprocessed paragraphs\n",
        "        self.vectorizer.fit(preprocessed_paragraphs)\n",
        "\n",
        "        # Transform the preprocessed paragraphs into vectors\n",
        "\n",
        "        paragraph_vectors = self.vectorizer.transform(preprocessed_paragraphs)\n",
        "        paragraph_embeddings = self.sentenceEncoder_model(preprocessed_paragraphs)\n",
        "\n",
        "        most_similar_ttps = []\n",
        "        most_similar_indexes = []\n",
        "        for i in range(len(paragraphs)):\n",
        "            paragraph_embedding = paragraph_embeddings[i]\n",
        "\n",
        "            # Compute the cosine similarity between the paragraph embedding and all TTP embeddings\n",
        "            similarities = cosine_similarity([paragraph_embedding], self.ttp_embeddings)\n",
        "\n",
        "            # Find the index of the most similar TTP\n",
        "            most_similar_index = np.argmax(similarities)\n",
        "\n",
        "            # Retrieve the most similar TTP\n",
        "            most_similar_ttp = self.ttps_Data.iloc[most_similar_index]\n",
        "            most_similar_ttps.append(most_similar_ttp)\n",
        "            most_similar_indexes.append(most_similar_index)\n",
        "\n",
        "        # Print the most similar TTPs for each paragraph\n",
        "        for i in range(len(paragraphs)):\n",
        "            if self.attackOrNot_Model.predict([paragraphs[i]])[0] == 0:\n",
        "                continue\n",
        "\n",
        "            print(\"\\nParagraph \", i, \" =========\\n\", paragraphs[i])\n",
        "            print(\"Tactic    :\", self.ttps_Data.iloc[most_similar_indexes[i]][1])\n",
        "            print(\"Technique :\", self.ttps_Data.iloc[most_similar_indexes[i]][2])\n",
        "            print(\"Procedure :\", self.ttps_Data.iloc[most_similar_indexes[i]][3])\n",
        "            print(\"Summarization --->\", self.summarize(paragraphs[i], 0.3))\n",
        "\n",
        "            sentences = paragraphs[i].split('.')\n",
        "            technique = self.ttps_Data.iloc[most_similar_indexes[i]][2]\n",
        "            procedure = self.ttps_Data.iloc[most_similar_indexes[i]][3]\n",
        "            print(\"Sentences for TTPs identification :\")\n",
        "            most_similar_technique=self.most_similar_string(technique, sentences)\n",
        "            if(most_similar_technique!=\"\"):\n",
        "              print(\"=========>>\", most_similar_technique)\n",
        "\n",
        "            print(\"=========>>\", self.most_similar_string(procedure, sentences))\n",
        "            print(\"------------------------------------------------\")\n",
        "\n",
        "    def Main(self):\n",
        "      print(\"Enter your choice \")\n",
        "      print(\"1:To load .txt file\")\n",
        "      print(\"2:To load paragraph\")\n",
        "      print(\"Press other key to exit\")\n",
        "      choice=int(input(\"Input choice: \"))\n",
        "      while(choice==1 or choice==2):\n",
        "        if(choice==1):\n",
        "          location=input(\"Enter the location of file : \")\n",
        "          self.read_paragraphs_from_file(location)\n",
        "        else:\n",
        "          string=str(input(\"Enter the paragraph : \"))\n",
        "          self.vectorizationAndEmbedding([string])\n",
        "\n",
        "        choice=int(input(\"Enter the choice :\"))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = TTPS_prediction()\n"
      ],
      "metadata": {
        "id": "dSs0AG72GVZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.Main()\n"
      ],
      "metadata": {
        "id": "Vb3YjXElGbtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ySo3kac1HKto"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}