{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30jitPggxwt8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/content/drive/MyDrive/CyberSecurity/Task_3_TTPS/Complete_Data.csv')"
      ],
      "metadata": {
        "id": "vDggYSGs0fwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=data[['Tactic','Technique','Procedure']]"
      ],
      "metadata": {
        "id": "fHcdOd2L3RFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list1=data['Procedure'].tolist()"
      ],
      "metadata": {
        "id": "L_1PPdku3MIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list2=pd.read_csv(\"/content/drive/MyDrive/CyberSecurity/Task_3_TTPS/FineTunning.csv\")['0'].tolist()"
      ],
      "metadata": {
        "id": "tfGnZdM1Qb4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs=list1+list2\n",
        "labels_Attacks=[1 for i in range(0,len(paragraphs))]"
      ],
      "metadata": {
        "id": "g_YxOm7PVOTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/CyberSecurity/Task_3_TTPS/RandomParagraphs.txt\"\n",
        "file = open(file_path, \"r\")\n",
        "file_content = file.read()\n",
        "file.close()\n",
        "\n",
        "nonAttackParagraphs = file_content.split(\"\\n\\n\")\n",
        "labels_NAttacks=[0 for i in range(len(nonAttackParagraphs))]"
      ],
      "metadata": {
        "id": "zLHXo4UTZtet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs=paragraphs+nonAttackParagraphs\n",
        "labels=labels_Attacks+labels_NAttacks"
      ],
      "metadata": {
        "id": "MUcMSaDaaqnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from joblib import dump\n",
        "# Save the best model"
      ],
      "metadata": {
        "id": "g-6afspmznj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.utils import shuffle\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample data\n",
        "\n",
        "\n",
        "# Preprocessing steps\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'\\W+|\\d+', ' ', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize text\n",
        "    tokens = text.split()\n",
        "    # Remove stop words and lemmatize tokens\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    # Join tokens back to text\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Shuffle the data\n",
        "paragraphs, labels = shuffle(paragraphs, labels, random_state=42)\n",
        "\n",
        "# Preprocess the text data\n",
        "paragraphs = [preprocess_text(paragraph) for paragraph in paragraphs]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(paragraphs, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "parameters = {\n",
        "    'tfidf__max_features': [1000, 5000, 10000],\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    'classifier__n_estimators': [100, 200, 300],\n",
        "    'classifier__max_depth': [None, 10, 20],\n",
        "    'classifier__min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Perform grid search for hyperparameter tuning\n",
        "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='f1')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best F1 Score:\", grid_search.best_score_)\n",
        "\n",
        "# Get the best model from the grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "SnkVDGAkx3xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_filename = \"AttackOrNot.joblib\"\n",
        "dump(best_model, model_filename)"
      ],
      "metadata": {
        "id": "s77jS7FgzujR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "from heapq import nlargest\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ],
      "metadata": {
        "id": "7F_37s6FkGrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text, per):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc= nlp(text)\n",
        "    tokens=[token.text for token in doc]\n",
        "    word_frequencies={}\n",
        "    for word in doc:\n",
        "        if word.text.lower() not in list(STOP_WORDS):\n",
        "            if word.text.lower() not in punctuation:\n",
        "                if word.text not in word_frequencies.keys():\n",
        "                    word_frequencies[word.text] = 1\n",
        "                else:\n",
        "                    word_frequencies[word.text] += 1\n",
        "    max_frequency=max(word_frequencies.values())\n",
        "    for word in word_frequencies.keys():\n",
        "        word_frequencies[word]=word_frequencies[word]/max_frequency\n",
        "    sentence_tokens= [sent for sent in doc.sents]\n",
        "    sentence_scores = {}\n",
        "    for sent in sentence_tokens:\n",
        "        for word in sent:\n",
        "            if word.text.lower() in word_frequencies.keys():\n",
        "                if sent not in sentence_scores.keys():\n",
        "                    sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
        "                else:\n",
        "                    sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
        "    select_length=int(len(sentence_tokens)*per)\n",
        "    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
        "    final_summary=[word.text for word in summary]\n",
        "    summary=''.join(final_summary)\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "0FVZgtDmEjAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/CyberSecurity/Task_3_TTPS/threat.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    file_content = file.read()\n",
        "\n",
        "testParaSummarization = file_content.split(\"\\n\\n\")\n"
      ],
      "metadata": {
        "id": "h_UVMA3tl6jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "c1lFcAsPz0hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_paragraphs_from_file(file_path):\n",
        "    with open(file_path, 'r',encoding=\"UTF-8\") as file:\n",
        "        content = file.read()\n",
        "        paragraphs = content.split('\\n\\n')\n",
        "        paragraphs = [paragraph.strip() for paragraph in paragraphs if paragraph.strip() and len(paragraph.split())<=400]\n",
        "    return paragraphs\n",
        "\n",
        "# Provide the file path of the text file\n",
        "file_path = '/content/drive/MyDrive/CyberSecurity/Task_3_TTPS/threat.txt'\n",
        "paragraphs_clean = read_paragraphs_from_file(file_path)\n",
        "paragraphs=paragraphs_clean.copy()\n",
        "\n"
      ],
      "metadata": {
        "id": "VbA3txaBhUfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import joblib\n",
        "# Step 1: Load TTPs from the CSV file\n",
        "ttps_df = pd.read_csv(\"/content/drive/MyDrive/CyberSecurity/Task_3_TTPS/Complete_Data.csv\")\n",
        "ttps = ttps_df[\"Procedure\"].tolist()\n",
        "\n",
        "\n",
        "# Step 2: Load paragraphs from the text file\n",
        "# with open(\"threat.txt\", \"r\") as file:\n",
        "#     paragraphs = file.readlines()\n",
        "\n",
        "# Step 3: Preprocess TTPs and paragraphs\n",
        "\n",
        "# Preprocessing function for removing unnecessary characters or formatting\n",
        "def preprocess_text(text):\n",
        "    # Apply any preprocessing steps (e.g., removing special characters, lowercasing)\n",
        "    string = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Replace email addresses with 'email'\n",
        "    email_regex = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b')\n",
        "    string = email_regex.sub('email', string)\n",
        "\n",
        "    # Replace IP addresses with 'ip'\n",
        "    ip_regex = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n",
        "    string = ip_regex.sub('ip', string)\n",
        "\n",
        "    # Remove newline characters\n",
        "    string = string.replace('\\n', '')\n",
        "    soup = BeautifulSoup(string, \"html.parser\")\n",
        "    string = soup.get_text()\n",
        "\n",
        "    # Remove special characters\n",
        "    string = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", string)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    string = re.sub(r\"\\s+\", \" \", string)\n",
        "\n",
        "    preprocessed_text = string.strip()  # Remove leading/trailing whitespaces\n",
        "    return preprocessed_text\n",
        "\n",
        "preprocessed_ttps = [preprocess_text(ttp) for ttp in ttps]\n",
        "preprocessed_paragraphs = [preprocess_text(paragraph) for paragraph in paragraphs]\n",
        "\n",
        "# Step 4: Vectorize TTPs and paragraphs using TF-IDF\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer on the preprocessed TTPs\n",
        "ttp_vectors = vectorizer.fit_transform(preprocessed_ttps)\n",
        "\n",
        "# Transform the preprocessed paragraphs into vectors\n",
        "paragraphs = [paragraph.strip() for paragraph in paragraphs]  # Remove leading/trailing whitespaces\n",
        "paragraph_vectors = vectorizer.transform(preprocessed_paragraphs)\n",
        "\n",
        "# Step 5: Generate sentence embeddings using Universal Sentence Encoder\n",
        "\n",
        "# Load the Universal Sentence Encoder model\n",
        "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "# Encode TTPs and paragraphs to get sentence embeddings\n",
        "ttp_embeddings = use_model(preprocessed_ttps)\n",
        "paragraph_embeddings = use_model(preprocessed_paragraphs)\n",
        "\n",
        "# Step 6: Find the most similar TTP for each paragraph\n",
        "#joblib.dump(ttp_embeddings, \"ttp_embeddings.pkl\")\n",
        "\n",
        "most_similar_ttps = []\n",
        "most_similar_indexes=[]\n",
        "# Iterate over each paragraph\n",
        "for i in range(len(paragraphs)):\n",
        "    paragraph_embedding = paragraph_embeddings[i]\n",
        "\n",
        "    # Compute the cosine similarity between the paragraph embedding and all TTP embeddings\n",
        "    similarities = cosine_similarity([paragraph_embedding], ttp_embeddings)\n",
        "\n",
        "    # Find the index of the most similar TTP\n",
        "    most_similar_index = np.argmax(similarities)\n",
        "\n",
        "    # Retrieve the most similar TTP\n",
        "    most_similar_ttp = ttps[most_similar_index]\n",
        "    most_similar_ttps.append(most_similar_ttp)\n",
        "    most_similar_indexes.append(most_similar_index)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oHyzN83ThgxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib"
      ],
      "metadata": {
        "id": "zk4vCUerkDzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs"
      ],
      "metadata": {
        "id": "pzsWm3bzlQ52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attackModel=joblib.load(\"/content/drive/MyDrive/CyberSecurity/Task_3_TTPS/AttackOrNot.joblib\")\n"
      ],
      "metadata": {
        "id": "0oAwRcC1niSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def most_similar_string(sentence, string_list):\n",
        "  \"\"\"\n",
        "  Returns the most similar string in the string list to the given sentence.\n",
        "\n",
        "  Args:\n",
        "    sentence: The sentence to compare.\n",
        "    string_list: A list of strings.\n",
        "\n",
        "  Returns:\n",
        "    The most similar string in the string list.\n",
        "  \"\"\"\n",
        "\n",
        "  tfidf_vectorizer = TfidfVectorizer()\n",
        "  tfidf_matrix = tfidf_vectorizer.fit_transform(string_list + [sentence])\n",
        "\n",
        "  use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "  sentence_embeddings = use_model([sentence])[0]\n",
        "  string_embeddings = use_model(string_list)\n",
        "\n",
        "  cosine_similarities = cosine_similarity([sentence_embeddings], string_embeddings)[0]\n",
        "\n",
        "  most_similar_index = np.argmax(cosine_similarities)\n",
        "  return string_list[most_similar_index]\n"
      ],
      "metadata": {
        "id": "Iv26gk0FrYqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attackOrNot_predict=attackModel.predict(paragraphs)"
      ],
      "metadata": {
        "id": "3CJEtDbjpHOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(paragraphs)):\n",
        "  if(attackOrNot_predict[i]==0):\n",
        "    continue\n",
        "\n",
        "  print(\"\\nParagraph \",i,\" ==>\\n\",paragraphs[i])\n",
        "  print(\"Tactic    :\",ttps_df.iloc[most_similar_indexes[i]][1])\n",
        "  print(\"Techique  :\",ttps_df.iloc[most_similar_indexes[i]][2])\n",
        "  print(\"Procedure :\",ttps_df.iloc[most_similar_indexes[i]][3])\n",
        "  print(\"Summarization --->\",summarize(paragraphs[i],0.3))\n",
        "\n",
        "  sentences=paragraphs[i].split('.')\n",
        "  techniques=ttps_df.iloc[most_similar_indexes[i]][2]\n",
        "  procedure=ttps_df.iloc[most_similar_indexes[i]][3]\n",
        "  print(\"Sentences for TTPs identification :\")\n",
        "  print(\"=========>>\",most_similar_string(techniques,sentences))\n",
        "  print(\"=========>>\",most_similar_string(procedure,sentences))\n",
        "  print(\"------------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "DGmuwNa7N10b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import tensorflow_hub as hub\n",
        "from heapq import nlargest\n",
        "from string import punctuation"
      ],
      "metadata": {
        "id": "j4OBDgfilwv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TTPS_prediction:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.ttps_Data = pd.read_csv(\"/content/drive/MyDrive/CyberSecurity/Task_3_TTPS/Complete_Data.csv\")\n",
        "        self.sentenceEncoder_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "        self.attackOrNot_Model = joblib.load(\"/content/drive/MyDrive/CyberSecurity/Task_3_TTPS/AttackOrNot.joblib\")\n",
        "        self.ttp_embeddings = joblib.load(\"/content/drive/MyDrive/CyberSecurity/Task_3_TTPS/ttp_embeddings.pkl\")\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "\n",
        "    def read_paragraphs_from_file(self,file_path):\n",
        "        try:\n",
        "          with open(file_path, 'r',encoding=\"UTF-8\") as file:\n",
        "              content = file.read()\n",
        "              paragraphs = content.split('\\n\\n')\n",
        "              paragraphs = [paragraph.strip() for paragraph in paragraphs if paragraph.strip() and len(paragraph.split())<=400]\n",
        "          self.vectorizationAndEmbedding(paragraphs)\n",
        "        except :\n",
        "          print(\"XXXXXXXXXXXXXX File is not present at give path XXXXXXXXXXX\")\n",
        "\n",
        "\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        # Apply any preprocessing steps (e.g., removing special characters, lowercasing)\n",
        "        string = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "        # Replace email addresses with 'email'\n",
        "        email_regex = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b')\n",
        "        string = email_regex.sub('email', string)\n",
        "\n",
        "        # Replace IP addresses with 'ip'\n",
        "        ip_regex = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n",
        "        string = ip_regex.sub('ip', string)\n",
        "\n",
        "        # Remove newline characters\n",
        "        string = string.replace('\\n', '')\n",
        "        soup = BeautifulSoup(string, \"html.parser\")\n",
        "        string = soup.get_text()\n",
        "\n",
        "        # Remove special characters\n",
        "        string = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", string)\n",
        "\n",
        "        # Remove extra whitespaces\n",
        "        string = re.sub(r\"\\s+\", \" \", string)\n",
        "\n",
        "        preprocessed_text = string.strip()  # Remove leading/trailing whitespaces\n",
        "        return preprocessed_text\n",
        "\n",
        "    def summarize(self, text, per):\n",
        "        doc = self.nlp(text)\n",
        "        tokens = [token.text for token in doc]\n",
        "        word_frequencies = {}\n",
        "        for word in doc:\n",
        "            if word.text.lower() not in list(STOP_WORDS):\n",
        "                if word.text.lower() not in punctuation:\n",
        "                    if word.text not in word_frequencies.keys():\n",
        "                        word_frequencies[word.text] = 1\n",
        "                    else:\n",
        "                        word_frequencies[word.text] += 1\n",
        "        max_frequency = max(word_frequencies.values())\n",
        "        for word in word_frequencies.keys():\n",
        "            word_frequencies[word] = word_frequencies[word] / max_frequency\n",
        "        sentence_tokens = [sent for sent in doc.sents]\n",
        "        sentence_scores = {}\n",
        "        for sent in sentence_tokens:\n",
        "            for word in sent:\n",
        "                if word.text.lower() in word_frequencies.keys():\n",
        "                    if sent not in sentence_scores.keys():\n",
        "                        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
        "                    else:\n",
        "                        sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
        "        select_length = int(len(sentence_tokens) * per)\n",
        "        summary = nlargest(select_length, sentence_scores, key=sentence_scores.get)\n",
        "        final_summary = [word.text for word in summary]\n",
        "        summary = ''.join(final_summary)\n",
        "        return summary\n",
        "\n",
        "    def most_similar_string(self, sentence, string_list):\n",
        "        \"\"\"\n",
        "        Returns the most similar string in the string list to the given sentence.\n",
        "\n",
        "        Args:\n",
        "          sentence: The sentence to compare.\n",
        "          string_list: A list of strings.\n",
        "\n",
        "        Returns:\n",
        "          The most similar string in the string list.\n",
        "        \"\"\"\n",
        "\n",
        "        tfidf_matrix = self.vectorizer.transform(string_list + [sentence])\n",
        "\n",
        "        sentence_embeddings = self.sentenceEncoder_model([sentence])[0]\n",
        "        string_embeddings = self.sentenceEncoder_model(string_list)\n",
        "\n",
        "        cosine_similarities = cosine_similarity([sentence_embeddings], string_embeddings)[0]\n",
        "\n",
        "        most_similar_index = np.argmax(cosine_similarities)\n",
        "        return string_list[most_similar_index]\n",
        "\n",
        "    def vectorizationAndEmbedding(self, paragraphs):\n",
        "        preprocessed_paragraphs = [self.preprocess_text(paragraph) for paragraph in paragraphs]\n",
        "\n",
        "        # Fit the vectorizer on the preprocessed paragraphs\n",
        "        self.vectorizer.fit(preprocessed_paragraphs)\n",
        "\n",
        "        # Transform the preprocessed paragraphs into vectors\n",
        "\n",
        "        paragraph_vectors = self.vectorizer.transform(preprocessed_paragraphs)\n",
        "        paragraph_embeddings = self.sentenceEncoder_model(preprocessed_paragraphs)\n",
        "\n",
        "        most_similar_ttps = []\n",
        "        most_similar_indexes = []\n",
        "        for i in range(len(paragraphs)):\n",
        "            paragraph_embedding = paragraph_embeddings[i]\n",
        "\n",
        "            # Compute the cosine similarity between the paragraph embedding and all TTP embeddings\n",
        "            similarities = cosine_similarity([paragraph_embedding], self.ttp_embeddings)\n",
        "\n",
        "            # Find the index of the most similar TTP\n",
        "            most_similar_index = np.argmax(similarities)\n",
        "\n",
        "            # Retrieve the most similar TTP\n",
        "            most_similar_ttp = self.ttps_Data.iloc[most_similar_index]\n",
        "            most_similar_ttps.append(most_similar_ttp)\n",
        "            most_similar_indexes.append(most_similar_index)\n",
        "\n",
        "        # Print the most similar TTPs for each paragraph\n",
        "        for i in range(len(paragraphs)):\n",
        "            if self.attackOrNot_Model.predict([paragraphs[i]])[0] == 0:\n",
        "                continue\n",
        "\n",
        "            print(\"\\nParagraph \", i, \" =========\\n\", paragraphs[i])\n",
        "            print(\"Tactic    :\", self.ttps_Data.iloc[most_similar_indexes[i]][1])\n",
        "            print(\"Technique :\", self.ttps_Data.iloc[most_similar_indexes[i]][2])\n",
        "            print(\"Procedure :\", self.ttps_Data.iloc[most_similar_indexes[i]][3])\n",
        "            print(\"Summarization --->\", self.summarize(paragraphs[i], 0.3))\n",
        "\n",
        "            sentences = paragraphs[i].split('.')\n",
        "            technique = self.ttps_Data.iloc[most_similar_indexes[i]][2]\n",
        "            procedure = self.ttps_Data.iloc[most_similar_indexes[i]][3]\n",
        "            print(\"Sentences for TTPs identification :\")\n",
        "            most_similar_technique=self.most_similar_string(technique, sentences)\n",
        "            if(most_similar_technique!=\"\"):\n",
        "              print(\"=========>>\", most_similar_technique)\n",
        "\n",
        "            print(\"=========>>\", self.most_similar_string(procedure, sentences))\n",
        "            print(\"------------------------------------------------\")\n",
        "\n",
        "    def Main(self):\n",
        "      print(\"Enter your choice \")\n",
        "      print(\"1:To load .txt file\")\n",
        "      print(\"2:To load paragraph\")\n",
        "      print(\"Press other key to exit\")\n",
        "      choice=int(input(\"Input choice: \"))\n",
        "      while(choice==1 or choice==2):\n",
        "        if(choice==1):\n",
        "          location=input(\"Enter the location of file : \")\n",
        "          self.read_paragraphs_from_file(location)\n",
        "        else:\n",
        "          string=str(input(\"Enter the paragraph : \"))\n",
        "          self.vectorizationAndEmbedding([string])\n",
        "\n",
        "        choice=int(input(\"Enter the choice :\"))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5jaSjJpIz5wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = TTPS_prediction()"
      ],
      "metadata": {
        "id": "VZdA6yFlrdwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.Main()"
      ],
      "metadata": {
        "id": "kaKQ18QBrdrQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}